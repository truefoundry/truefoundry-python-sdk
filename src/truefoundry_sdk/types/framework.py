# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations
from ..core.pydantic_utilities import UniversalBaseModel
import typing
from .library_name import LibraryName
from ..core.pydantic_utilities import IS_PYDANTIC_V2
import pydantic
from .sklearn_serialization_format import SklearnSerializationFormat
from .sklearn_model_schema import SklearnModelSchema
from .xg_boost_serialization_format import XgBoostSerializationFormat
from .xg_boost_model_schema import XgBoostModelSchema


class Framework_Transformers(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["transformers"] = "transformers"
    library_name: typing.Optional[LibraryName] = None
    pipeline_tag: typing.Optional[str] = None
    base_model: typing.Optional[str] = None

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Tensorflow(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["tensorflow"] = "tensorflow"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Sklearn(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["sklearn"] = "sklearn"
    model_filepath: typing.Optional[str] = None
    serialization_format: typing.Optional[SklearnSerializationFormat] = None
    model_schema: typing.Optional[SklearnModelSchema] = None

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Pytorch(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["pytorch"] = "pytorch"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Keras(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["keras"] = "keras"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Xgboost(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["xgboost"] = "xgboost"
    serialization_format: typing.Optional[XgBoostSerializationFormat] = None
    model_filepath: typing.Optional[str] = None
    model_schema: typing.Optional[XgBoostModelSchema] = None

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Lightgbm(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["lightgbm"] = "lightgbm"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Fastai(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["fastai"] = "fastai"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_H2O(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["h2o"] = "h2o"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Onnx(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["onnx"] = "onnx"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Spacy(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["spacy"] = "spacy"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Statsmodels(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["statsmodels"] = "statsmodels"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Gluon(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["gluon"] = "gluon"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


class Framework_Paddle(UniversalBaseModel):
    """
    Framework for the model version like Transformers, PyTorch, Sklearn, Xgboost etc with framework specific metadata. This will be used to infer model deployment configuration
    """

    type: typing.Literal["paddle"] = "paddle"

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow")  # type: ignore # Pydantic v2
    else:

        class Config:
            smart_union = True
            extra = pydantic.Extra.allow


Framework = typing.Union[
    Framework_Transformers,
    Framework_Tensorflow,
    Framework_Sklearn,
    Framework_Pytorch,
    Framework_Keras,
    Framework_Xgboost,
    Framework_Lightgbm,
    Framework_Fastai,
    Framework_H2O,
    Framework_Onnx,
    Framework_Spacy,
    Framework_Statsmodels,
    Framework_Gluon,
    Framework_Paddle,
]
